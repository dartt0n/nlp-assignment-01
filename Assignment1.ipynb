{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIgM6C9HYUhm"
   },
   "source": [
    "# Context-sensitive Spelling Correction\n",
    "\n",
    "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
    "\n",
    "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
    "\n",
    "Useful links:\n",
    "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
    "- [Norvig's dataset](https://norvig.com/big.txt)\n",
    "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
    "\n",
    "Grading:\n",
    "- 60 points - Implement spelling correction\n",
    "- 20 points - Justify your decisions\n",
    "- 20 points - Evaluate on a test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-vb8yFOGRDF"
   },
   "source": [
    "## Implement context-sensitive spelling correction\n",
    "\n",
    "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
    "\n",
    "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
    "\n",
    "When solving this task, we expect you'll face (and successfully deal with) some problems or make up the ideas of the model improvement. Some of them are: \n",
    "\n",
    "- solving a problem of n-grams frequencies storing for a large corpus;\n",
    "- taking into account keyboard layout and associated misspellings;\n",
    "- efficiency improvement to make the solution faster;\n",
    "- ...\n",
    "\n",
    "Please don't forget to describe such cases, and what you decided to do with them, in the Justification section.\n",
    "\n",
    "##### IMPORTANT:  \n",
    "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
    "- Your implementation\n",
    "- Analysis of why the implemented approach is suggested\n",
    "- Improvements of the original approach that you have chosen to implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oML-5sJwGRLE"
   },
   "source": [
    "## Justify your decisions\n",
    "\n",
    "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
    "- Which ngram dataset to use\n",
    "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
    "- Beam search parameters\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46rk65S4GRSe"
   },
   "source": [
    "## Evaluate on a test set\n",
    "\n",
    "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity (or just take another dataset). Compare your solution to the Norvig's corrector, and report the accuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useful resources (also included in the archive in moodle):\n",
    "\n",
    "1. [Possible dataset with N-grams](https://www.ngrams.info/download_coca.asp)\n",
    "2. [Damerau–Levenshtein distance](https://en.wikipedia.org/wiki/Damerau–Levenshtein_distance#:~:text=Informally%2C%20the%20Damerau–Levenshtein%20distance,one%20word%20into%20the%20other.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution\n",
    "\n",
    "There are several approaches to solve this problem. \n",
    "\n",
    "## Approach 1: Sequence-to-Sequence Deep Learning Models\n",
    "These models are effective for capturing the context of the input text. They can complex patterns and relationships in the between words in a sentence and suggest better alternatives for a selected word. Moreover, they also could be trained on subword level and can be used for generating several options for a wrongly spelled word, eventually solving the out-of-vocabulary problem. However, as a downside, these models require a significant amount of data to train and are computationally expensive.\n",
    "\n",
    "## Approach 2: Word Embedding\n",
    "Models that produce word embeddings are trained to represent words as vectors in a high-dimensional space. These models are effective for capturing the context of the input text and represending it in a vector space. Once trained, they can be used to produce word embeddings for a given word based only on its context (skip-gram model). Spell correction could be done by producing vector search / clustering based on the word embeddings. However, these models are computationally expensive, requires additional vector space storage and prone to out-of-vocabulary problem (however, they are good at suggesting an alternative word based on the context).\n",
    "\n",
    "## Approach 3: Norvig Spell Correction\n",
    "This approach is based on the use of a dictionary of words and their corrections. It is simple and effective, but it is not very accurate. It is also quite slow.\n",
    "\n",
    "## Approach 4: . Symmetric Delete Spelling Correction Algorithm (SymSpell)\n",
    "This algorithm is known for its speed and efficiency. It reduces the complexity of edit candidate generation by only considering deletions, making it language-independent and fast. It is significantly faster than traditional algorithms like Norvig's or BK-tree, making it suitable for large-scale applications. However, as a downside, it does not capture complex contextual relationships between words, which can lead to inaccurate suggestions.\n",
    "\n",
    "## Approach 5: Noisy Channel\n",
    "These models use probabilistic approaches to correct misspellings by maximizing the probability of the intended word given the misspelled word. They can handle various types of errors, including phonetic and keyboard errors. However, they require accurate models of both word probabilities and error channels.\n",
    "\n",
    "\n",
    "|Approach|Speed|Contextual Understanding|Complexity|\n",
    "|---|---|---|---|\n",
    "|Deep Learning|Slow|High|High|\n",
    "|Word Embedding|Medium|Medium|Medium|\n",
    "|Norvig|Medium|Low|Low|\n",
    "|SymSpell|Fast|Low|Low|\n",
    "|Noisy Channel|Medium|Medium|Medium|\n",
    "\n",
    "In this notebook, I will implement Approach 5 - Noisy Channel. Details of the approach can be found here: [Spelling Correction and the Noisy Channel](https://web.stanford.edu/~jurafsky/slp3/B.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data is takes from [1 Billion Word Language Model Benchmark](https://www.statmt.org/lm-benchmark/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "class NoisyChannelSpellCorrector:\n",
    "    def __init__(self):\n",
    "        self.word_counts = Counter()\n",
    "        self.bigram_counts = defaultdict(int)\n",
    "        self.vocab = set()\n",
    "        self.total_words = 0\n",
    "        self.alphabet = string.ascii_lowercase\n",
    "        self.similar_words_cache = {}\n",
    "\n",
    "        self.qwerty_distances = self._init_qwerty_distances()\n",
    "\n",
    "    def _init_qwerty_distances(self) -> dict[Tuple[str, str], float]:\n",
    "        keyboard_layout = [\"qwertyuiop\", \"asdfghjkl\", \"zxcvbnm\"]\n",
    "\n",
    "        char_positions = {}\n",
    "        for row_idx, row in enumerate(keyboard_layout):\n",
    "            for col_idx, char in enumerate(row):\n",
    "                char_positions[char] = (row_idx, col_idx)\n",
    "\n",
    "        distances = {}\n",
    "        for c1 in self.alphabet:\n",
    "            for c2 in self.alphabet:\n",
    "                if c1 not in char_positions or c2 not in char_positions:\n",
    "                    distances[(c1, c2)] = 1.0\n",
    "                    continue\n",
    "\n",
    "                pos1 = char_positions[c1]\n",
    "                pos2 = char_positions[c2]\n",
    "                distance = ((pos1[0] - pos2[0]) ** 2 + (pos1[1] - pos2[1]) ** 2) ** 0.5\n",
    "                distances[(c1, c2)] = min(distance / 5.0, 1.0)\n",
    "\n",
    "        return distances\n",
    "\n",
    "    def _levenshtein_distance(self, s1: str, s2: str) -> float:\n",
    "        if len(s1) < len(s2):\n",
    "            return self._levenshtein_distance(s2, s1)\n",
    "\n",
    "        if len(s2) == 0:\n",
    "            return len(s1)\n",
    "\n",
    "        previous_row = range(len(s2) + 1)\n",
    "        for i, c1 in enumerate(s1):\n",
    "            current_row = [i + 1]\n",
    "            for j, c2 in enumerate(s2):\n",
    "                keyboard_dist = self.qwerty_distances.get((c1, c2), 1.0)\n",
    "                insertions = previous_row[j + 1] + 1\n",
    "                deletions = current_row[j] + 1\n",
    "                substitutions = previous_row[j] + keyboard_dist\n",
    "                current_row.append(min(insertions, deletions, substitutions))\n",
    "            previous_row = current_row\n",
    "\n",
    "        return previous_row[-1]\n",
    "\n",
    "    def train(self, file_paths: list[str | Path]):\n",
    "        words = []\n",
    "\n",
    "        for file_path in file_paths:\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                num_lines = sum(1 for _ in f)\n",
    "\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in tqdm(f, desc=f\"Processing file: {file_path}\", total=num_lines):\n",
    "                    words.extend(nltk.word_tokenize(line.lower()))\n",
    "\n",
    "        self.total_words = len(words)\n",
    "        self.vocab = set(words)\n",
    "        self.word_counts.update(words)\n",
    "\n",
    "        padded_words = [\"<s>\"] + words + [\"</s>\"]\n",
    "        for i in tqdm(\n",
    "            range(len(padded_words) - 1), desc=\"Building bigrams\", unit=\"bigrams\"\n",
    "        ):\n",
    "            bigram = tuple(padded_words[i : i + 2])\n",
    "            self.bigram_counts[bigram] += 1\n",
    "\n",
    "    def _get_similar_words(self, word: str) -> set[str]:\n",
    "        if word in self.similar_words_cache:\n",
    "            return self.similar_words_cache[word]\n",
    "\n",
    "        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "        deletes = [L + R[1:] for L, R in splits if R]\n",
    "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "        replaces = [L + c + R[1:] for L, R in splits if R for c in self.alphabet]\n",
    "        inserts = [L + c + R for L, R in splits for c in self.alphabet]\n",
    "\n",
    "        candidates = set(deletes + transposes + replaces + inserts)\n",
    "        self.similar_words_cache[word] = candidates\n",
    "        return candidates\n",
    "\n",
    "    def _channel_probability(self, error: str, correction: str) -> float:\n",
    "        if error == correction:\n",
    "            return 1.0\n",
    "\n",
    "        lev_distance = self._levenshtein_distance(error, correction)\n",
    "        similarity = 1.0 / (1.0 + lev_distance)\n",
    "\n",
    "        if correction in self._get_similar_words(error):\n",
    "            similarity *= 2.0\n",
    "\n",
    "        return similarity\n",
    "\n",
    "    def _language_model_probability(self, prev_word: str, word: str) -> float:\n",
    "        bigram = tuple([prev_word, word])\n",
    "        bigram_count = self.bigram_counts[bigram]\n",
    "        context_count = self.word_counts[prev_word]\n",
    "\n",
    "        word_prob = self.word_counts[word] / self.total_words\n",
    "\n",
    "        if context_count == 0:\n",
    "            return word_prob\n",
    "\n",
    "        contextual_prob = (bigram_count + 1) / (context_count + len(self.vocab))\n",
    "        return contextual_prob * 2 + word_prob\n",
    "\n",
    "    def _correct_word(self, prev_word: str, word: str) -> str:\n",
    "        if word in self.vocab:\n",
    "            return word\n",
    "\n",
    "        candidates = self._get_similar_words(word)\n",
    "        candidates = candidates.intersection(self.vocab)\n",
    "\n",
    "        if not candidates:\n",
    "            return word\n",
    "\n",
    "        best_prob = float(\"-inf\")\n",
    "        best_correction = word\n",
    "\n",
    "        for candidate in candidates:\n",
    "            channel_prob = self._channel_probability(word, candidate)\n",
    "            lang_prob = self._language_model_probability(prev_word, candidate)\n",
    "\n",
    "            prob = 0.4 * channel_prob + 0.6 * lang_prob\n",
    "\n",
    "            if prob > best_prob:\n",
    "                best_prob = prob\n",
    "                best_correction = candidate\n",
    "\n",
    "        return best_correction\n",
    "\n",
    "    def correct(self, text: str) -> str:\n",
    "        words = nltk.word_tokenize(text.lower())\n",
    "        corrected_words = []\n",
    "\n",
    "        prev_word = \"<s>\"\n",
    "        for word in words:\n",
    "            if word in self.vocab:\n",
    "                corrected = word\n",
    "            else:\n",
    "                corrected = self._correct_word(prev_word, word)\n",
    "\n",
    "            corrected_words.append(corrected)\n",
    "            prev_word = corrected\n",
    "\n",
    "        return \" \".join(corrected_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"https://storage.yandexcloud.net/dartt0n/share/text-corpus-001-010.txt\" # 420MB\n",
    "# !wget \"https://storage.yandexcloud.net/dartt0n/share/text-corpus-001-050.txt\" # 2.1GB\n",
    "# !wget \"https://storage.yandexcloud.net/dartt0n/share/text-corpus-001-100.txt\" # 4.1GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing file: text-corpus-001-010.txt:   1%|          | 16204/3062393 [00:00<02:19, 21760.35it/s]"
     ]
    }
   ],
   "source": [
    "corrector = NoisyChannelSpellCorrector()\n",
    "corrector.train([\"text-corpus-001-010.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this jxb is done -> this jxb is done\n",
      "dking species -> eking species\n",
      "dking sport -> eking sport\n",
      "doing spxrt -> doing spart\n",
      "pizza is tastx -> pizza is taste\n",
      "vwry beautiful -> very beautiful\n",
      "i lkke nlp -> i like nlp\n",
      "prxjxcts are good -> prxjxcts are good\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"this jxb is done\",\n",
    "    \"dking species\",\n",
    "    \"dking sport\",\n",
    "    \"doing spxrt\",\n",
    "    \"pizza is tastx\",\n",
    "    \"vwry beautiful\",\n",
    "    \"i lkke nlp\",\n",
    "    \"prxjxcts are good\",\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    corrected = corrector.correct(text)\n",
    "    print(f\"{text} -> {corrected}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Differences\n",
    "Key differences and benefits compared to Norvig's algorithm:\n",
    "\n",
    "### Context-Aware Corrections (Bigram Model)\n",
    "   Norvig:\n",
    "   - Uses unigram frequencies only\n",
    "   - P(correction) = count(word) / total_words\n",
    "   \n",
    "   This model:\n",
    "   - Uses bigram frequencies\n",
    "   - P(correction|previous_word) = count(prev_word, word) / count(prev_word)\n",
    "   Benefit: Can distinguish between valid words in wrong context\n",
    "   Example: \"this job is dine\" → \"done\" (even though \"dine\" is valid)\n",
    "\n",
    "### Weighted Channel Model\n",
    "   Norvig:\n",
    "   - Simple edit distance (0, 1, 2)\n",
    "   - Equal weights for all edits\n",
    "   \n",
    "   This model:\n",
    "   - Keyboard distance-aware edits\n",
    "   - Levenshtein with weighted substitutions\n",
    "   - P(error|correction) = 1 / (1 + weighted_distance)\n",
    "   Benefit: More realistic error modeling\n",
    "   Example: \"vwry\" → \"very\" (higher prob than \"vary\" due to keyboard)\n",
    "\n",
    "### Combined Probability Model\n",
    "   Norvig:\n",
    "   - P(correction) * P(error|correction)\n",
    "   \n",
    "   This model:\n",
    "   - 0.4 * channel_prob + 0.6 * language_prob\n",
    "   - Allows tuning for different error types\n",
    "   Benefit: Better balance between edit distance and context\n",
    "   Example: Can adjust weights for typing vs context errors\n",
    "\n",
    "### Vocabulary Handling\n",
    "   Norvig:\n",
    "   - Tries to correct every word\n",
    "   \n",
    "   This model:\n",
    "   - Preserves dictionary words unless context suggests otherwise\n",
    "   - if word in vocab: return word\n",
    "   Benefit: Faster and preserves correct but rare words\n",
    "   Example: Proper nouns and valid but uncommon words stay unchanged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-world benefits:\n",
    "\n",
    "### Better Accuracy for Common Typos\n",
    "   - Keyboard proximity awareness helps with fat-finger errors\n",
    "   - Example: \"tge\" → \"the\" (higher probability due to 'g' being near 't')\n",
    "\n",
    "### Context-Sensitive Corrections\n",
    "   - Can fix grammatical errors that Norvig's can't\n",
    "   - Example: \"their goes\" → \"there goes\"\n",
    "\n",
    "### Performance Optimization\n",
    "   - Caching reduces repeated computations\n",
    "   - Dictionary word preservation reduces unnecessary checks"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
